{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70ea88d",
   "metadata": {},
   "source": [
    "#### Upload Data from S3 to PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8254f7",
   "metadata": {},
   "source": [
    "##### Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78363157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine, text\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "# For display\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16463e7d",
   "metadata": {},
   "source": [
    "#### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb2c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the data pipeline with configurations.\"\"\"\n",
    "        load_dotenv()\n",
    "        self.engine = self._create_db_connection()\n",
    "        self.bucket = \"finriskai\"\n",
    "        self.prefix = \"datasets/\"\n",
    "        self.files = self._get_s3_file_paths()\n",
    "        \n",
    "    def _create_db_connection(self):\n",
    "        \"\"\"Create PostgreSQL connection engine.\"\"\"\n",
    "        try:\n",
    "            db_url = (\n",
    "                f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@\"\n",
    "                f\"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "            )\n",
    "            engine = create_engine(db_url)\n",
    "            logger.info(\"Database connection established successfully\")\n",
    "            return engine\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create database connection: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_s3_file_paths(self) -> Dict[str, str]:\n",
    "        \"\"\"Define S3 file paths.\"\"\"\n",
    "        return {\n",
    "            \"applications\": f\"s3://{self.bucket}/{self.prefix}credit_applications.csv\",\n",
    "            \"bureau\": f\"s3://{self.bucket}/{self.prefix}credit_bureau_data.csv\",\n",
    "            \"profiles\": f\"s3://{self.bucket}/{self.prefix}customer_profiles.csv\",\n",
    "            \"predictions\": f\"s3://{self.bucket}/{self.prefix}model_predictions.csv\",\n",
    "            \"transactions\": f\"s3://{self.bucket}/{self.prefix}transaction_data.csv\",\n",
    "        }\n",
    "    \n",
    "    def load_data_from_s3(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load all datasets from S3 into DataFrames with error handling.\"\"\"\n",
    "        dataframes = {}\n",
    "        storage_options = {\"anon\": False}\n",
    "        \n",
    "        for name, path in self.files.items():\n",
    "            try:\n",
    "                logger.info(f\"Loading {name} from S3...\")\n",
    "                df = pd.read_csv(path, storage_options=storage_options)\n",
    "                logger.info(f\"Successfully loaded {name}: {len(df)} rows\")\n",
    "                dataframes[name] = df\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load {name} from S3: {e}\")\n",
    "                raise\n",
    "        \n",
    "        return dataframes\n",
    "    \n",
    "    def analyze_data_ranges(self, dataframes: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Analyze data ranges to set appropriate constraints.\"\"\"\n",
    "        if 'profiles' in dataframes:\n",
    "            df = dataframes['profiles']\n",
    "            logger.info(\"Data range analysis for profiles:\")\n",
    "            logger.info(f\"Behavioral score range: {df['behavioral_score'].min()} - {df['behavioral_score'].max()}\")\n",
    "            logger.info(f\"Credit score range: {df['credit_score'].min()} - {df['credit_score'].max()}\")\n",
    "            logger.info(f\"Customer age range: {df['customer_age'].min()} - {df['customer_age'].max()}\")\n",
    "            \n",
    "        if 'bureau' in dataframes:\n",
    "            df = dataframes['bureau']\n",
    "            logger.info(\"Data range analysis for bureau:\")\n",
    "            logger.info(f\"Credit utilization range: {df['credit_utilization'].min()} - {df['credit_utilization'].max()}\")\n",
    "            logger.info(f\"Payment history range: {df['payment_history'].min()} - {df['payment_history'].max()}\")\n",
    "\n",
    "    def drop_tables(self):\n",
    "        \"\"\"Drop existing tables to recreate with new schema.\"\"\"\n",
    "        tables_to_drop = [\n",
    "            \"fact_transactions\",\n",
    "            \"fact_predictions\", \n",
    "            \"fact_applications\",\n",
    "            \"dim_bureau\",\n",
    "            \"dim_customer_profiles\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                for table in tables_to_drop:\n",
    "                    logger.info(f\"Dropping table: {table}\")\n",
    "                    conn.execute(text(f\"DROP TABLE IF EXISTS {table} CASCADE;\"))\n",
    "                logger.info(\"All tables dropped successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to drop tables: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_tables(self):\n",
    "        \"\"\"Create database tables with customer_id as the key (PK + FK).\"\"\"\n",
    "        table_schemas = {\n",
    "            \"dim_customer_profiles\": \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS dim_customer_profiles (\n",
    "                    customer_id VARCHAR PRIMARY KEY,\n",
    "                    customer_age INT,\n",
    "                    annual_income NUMERIC(15,2),\n",
    "                    employment_status VARCHAR(50),\n",
    "                    account_tenure INT,\n",
    "                    product_holdings INT,\n",
    "                    relationship_value NUMERIC(15,2),\n",
    "                    risk_segment VARCHAR(50),\n",
    "                    behavioral_score NUMERIC(10,2),\n",
    "                    credit_score INT,\n",
    "                    city VARCHAR(100),\n",
    "                    last_activity_date DATE,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\",\n",
    "\n",
    "            \"dim_bureau\": \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS dim_bureau (\n",
    "                    customer_id VARCHAR PRIMARY KEY,\n",
    "                    credit_score INT,\n",
    "                    credit_history_length INT,\n",
    "                    number_of_accounts INT,\n",
    "                    total_credit_limit NUMERIC(15,2),\n",
    "                    credit_utilization NUMERIC(6,3),\n",
    "                    payment_history NUMERIC(6,3),\n",
    "                    public_records INT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    CONSTRAINT fk_bureau_customer FOREIGN KEY (customer_id)\n",
    "                        REFERENCES dim_customer_profiles(customer_id)\n",
    "                        ON DELETE CASCADE\n",
    "                );\n",
    "            \"\"\",\n",
    "\n",
    "            \"fact_applications\": \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS fact_applications (\n",
    "                    application_id VARCHAR PRIMARY KEY,\n",
    "                    customer_id VARCHAR,\n",
    "                    application_date DATE,\n",
    "                    loan_amount NUMERIC(15,2),\n",
    "                    loan_purpose VARCHAR(50),\n",
    "                    employment_status VARCHAR(50),\n",
    "                    annual_income NUMERIC(15,2),\n",
    "                    debt_to_income_ratio NUMERIC(6,3),\n",
    "                    credit_score INT,\n",
    "                    application_status VARCHAR(20),\n",
    "                    default_flag INT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    CONSTRAINT fk_app_customer FOREIGN KEY (customer_id)\n",
    "                        REFERENCES dim_customer_profiles(customer_id)\n",
    "                        ON DELETE CASCADE\n",
    "                );\n",
    "            \"\"\",\n",
    "\n",
    "            \"fact_predictions\": \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS fact_predictions (\n",
    "                    prediction_id VARCHAR PRIMARY KEY,\n",
    "                    model_version VARCHAR(50),\n",
    "                    customer_id VARCHAR,\n",
    "                    prediction_date DATE,\n",
    "                    prediction_type VARCHAR(30),\n",
    "                    risk_score NUMERIC(10,2),\n",
    "                    fraud_probability NUMERIC(6,3),\n",
    "                    model_features JSONB,\n",
    "                    prediction_explanation TEXT,\n",
    "                    business_decision VARCHAR(50),\n",
    "                    actual_outcome VARCHAR(50),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    CONSTRAINT fk_pred_customer FOREIGN KEY (customer_id)\n",
    "                        REFERENCES dim_customer_profiles(customer_id)\n",
    "                        ON DELETE CASCADE\n",
    "                );\n",
    "            \"\"\",\n",
    "\n",
    "            \"fact_transactions\": \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS fact_transactions (\n",
    "                    transaction_id VARCHAR PRIMARY KEY,\n",
    "                    customer_id VARCHAR,\n",
    "                    transaction_date TIMESTAMP,\n",
    "                    amount NUMERIC(15,2),\n",
    "                    merchant_category VARCHAR(50),\n",
    "                    transaction_type VARCHAR(30),\n",
    "                    location VARCHAR(100),\n",
    "                    device_info VARCHAR(200),\n",
    "                    fraud_flag INT,\n",
    "                    investigation_status VARCHAR(30),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    CONSTRAINT fk_txn_customer FOREIGN KEY (customer_id)\n",
    "                        REFERENCES dim_customer_profiles(customer_id)\n",
    "                        ON DELETE CASCADE\n",
    "                );\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                for table_name, schema in table_schemas.items():\n",
    "                    logger.info(f\"Creating table: {table_name}\")\n",
    "                    conn.execute(text(schema))\n",
    "                logger.info(\"All tables created successfully with customer_id as key\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create tables: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def create_indexes(self):\n",
    "        \"\"\"Create indexes for better query performance.\"\"\"\n",
    "        indexes = [\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_applications_customer_id ON fact_applications(customer_id);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_applications_date ON fact_applications(application_date);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_predictions_customer_id ON fact_predictions(customer_id);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_predictions_date ON fact_predictions(prediction_date);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_transactions_customer_id ON fact_transactions(customer_id);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_transactions_date ON fact_transactions(transaction_date);\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_profiles_city ON dim_customer_profiles(city);\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                for index in indexes:\n",
    "                    conn.execute(text(index))\n",
    "                logger.info(\"Indexes created successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create indexes: {e}\")\n",
    "            raise\n",
    "    \n",
    "    \n",
    "\n",
    "    def load_data_to_postgres(self, dataframes: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Load DataFrames to PostgreSQL with error handling and JSON conversion.\"\"\"\n",
    "        table_mapping = {\n",
    "            \"profiles\": \"dim_customer_profiles\",\n",
    "            \"bureau\": \"dim_bureau\",\n",
    "            \"applications\": \"fact_applications\",\n",
    "            \"predictions\": \"fact_predictions\",\n",
    "            \"transactions\": \"fact_transactions\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            for df_name, table_name in table_mapping.items():\n",
    "                if df_name in dataframes:\n",
    "                    df = dataframes[df_name]\n",
    "\n",
    "                    # ðŸ”¹ Fix JSON columns before loading\n",
    "                    if df_name == \"predictions\" and \"model_features\" in df.columns:\n",
    "                        df[\"model_features\"] = df[\"model_features\"].apply(\n",
    "                            lambda x: json.dumps(eval(x)) if isinstance(x, str) and x.startswith(\"{\") else json.dumps(x)\n",
    "                        )\n",
    "\n",
    "                    logger.info(f\"Loading {len(df)} records into {table_name}\")\n",
    "                    df.to_sql(\n",
    "                        table_name,\n",
    "                        con=self.engine,\n",
    "                        if_exists=\"append\",\n",
    "                        index=False,\n",
    "                        method=\"multi\",\n",
    "                        chunksize=1000\n",
    "                    )\n",
    "                    logger.info(f\"Successfully loaded {df_name}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data to PostgreSQL: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def validate_data_load(self):\n",
    "        \"\"\"Validate that data was loaded correctly.\"\"\"\n",
    "        validation_queries = {\n",
    "            \"dim_customer_profiles\": \"SELECT COUNT(*) as count FROM dim_customer_profiles;\",\n",
    "            \"dim_bureau\": \"SELECT COUNT(*) as count FROM dim_bureau;\",\n",
    "            \"fact_applications\": \"SELECT COUNT(*) as count FROM fact_applications;\",\n",
    "            \"fact_predictions\": \"SELECT COUNT(*) as count FROM fact_predictions;\",\n",
    "            \"fact_transactions\": \"SELECT COUNT(*) as count FROM fact_transactions;\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                for table, query in validation_queries.items():\n",
    "                    result = conn.execute(text(query)).fetchone()\n",
    "                    logger.info(f\"{table}: {result[0]} records loaded\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to validate data load: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_pipeline(self, force_recreate=True):\n",
    "        \"\"\"Execute the complete data pipeline.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting data pipeline...\")\n",
    "            \n",
    "            # Step 1: Load data from S3\n",
    "            dataframes = self.load_data_from_s3()\n",
    "            \n",
    "            # Step 2: Analyze data ranges\n",
    "            self.analyze_data_ranges(dataframes)\n",
    "            \n",
    "            # Step 3: Drop existing tables if force_recreate is True\n",
    "            if force_recreate:\n",
    "                logger.info(\"Force recreate enabled - dropping existing tables\")\n",
    "                self.drop_tables()\n",
    "            \n",
    "            # Step 4: Create tables\n",
    "            self.create_tables()\n",
    "            \n",
    "            # Step 5: Create indexes\n",
    "            self.create_indexes()\n",
    "            \n",
    "            # Step 6: Load data to PostgreSQL\n",
    "            self.load_data_to_postgres(dataframes)\n",
    "            \n",
    "            # Step 7: Validate data load\n",
    "            self.validate_data_load()\n",
    "            \n",
    "            logger.info(\"Data pipeline completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data pipeline failed: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c06ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 21:52:06,529 - INFO - Database connection established successfully\n",
      "2025-08-16 21:52:06,536 - INFO - Starting data pipeline...\n",
      "2025-08-16 21:52:06,541 - INFO - Loading applications from S3...\n",
      "2025-08-16 21:53:10,900 - INFO - Successfully loaded applications: 100000 rows\n",
      "2025-08-16 21:53:10,902 - INFO - Loading bureau from S3...\n",
      "2025-08-16 21:53:16,699 - INFO - Successfully loaded bureau: 25000 rows\n",
      "2025-08-16 21:53:16,700 - INFO - Loading profiles from S3...\n",
      "2025-08-16 21:53:29,553 - INFO - Successfully loaded profiles: 25000 rows\n",
      "2025-08-16 21:53:29,555 - INFO - Loading predictions from S3...\n",
      "2025-08-16 21:54:25,280 - INFO - Successfully loaded predictions: 50000 rows\n",
      "2025-08-16 21:54:25,280 - INFO - Loading transactions from S3...\n",
      "2025-08-16 21:55:09,661 - INFO - Successfully loaded transactions: 100000 rows\n",
      "2025-08-16 21:55:09,663 - INFO - Data range analysis for profiles:\n",
      "2025-08-16 21:55:09,685 - INFO - Behavioral score range: 2.19 - 927.24\n",
      "2025-08-16 21:55:09,689 - INFO - Credit score range: 379 - 850\n",
      "2025-08-16 21:55:09,692 - INFO - Customer age range: 18 - 247\n",
      "2025-08-16 21:55:09,696 - INFO - Data range analysis for bureau:\n",
      "2025-08-16 21:55:09,700 - INFO - Credit utilization range: 0.0 - 0.992\n",
      "2025-08-16 21:55:09,705 - INFO - Payment history range: 0.6 - 1.0\n",
      "2025-08-16 21:55:09,707 - INFO - Force recreate enabled - dropping existing tables\n",
      "2025-08-16 21:55:15,083 - INFO - Dropping table: fact_transactions\n",
      "2025-08-16 21:55:15,792 - INFO - Dropping table: fact_predictions\n",
      "2025-08-16 21:55:16,137 - INFO - Dropping table: fact_applications\n",
      "2025-08-16 21:55:16,443 - INFO - Dropping table: dim_bureau\n",
      "2025-08-16 21:55:16,785 - INFO - Dropping table: dim_customer_profiles\n",
      "2025-08-16 21:55:17,146 - INFO - All tables dropped successfully\n",
      "2025-08-16 21:55:17,555 - INFO - Creating table: dim_customer_profiles\n",
      "2025-08-16 21:55:18,388 - INFO - Creating table: dim_bureau\n",
      "2025-08-16 21:55:18,771 - INFO - Creating table: fact_applications\n",
      "2025-08-16 21:55:19,614 - INFO - Creating table: fact_predictions\n",
      "2025-08-16 21:55:19,955 - INFO - Creating table: fact_transactions\n",
      "2025-08-16 21:55:20,333 - INFO - All tables created successfully with customer_id as key\n",
      "2025-08-16 21:55:23,405 - INFO - Indexes created successfully\n",
      "2025-08-16 21:55:23,806 - INFO - Loading 25000 records into dim_customer_profiles\n",
      "2025-08-16 21:57:03,757 - INFO - Successfully loaded profiles\n",
      "2025-08-16 21:57:03,759 - INFO - Loading 25000 records into dim_bureau\n",
      "2025-08-16 21:57:46,180 - INFO - Successfully loaded bureau\n",
      "2025-08-16 21:57:46,182 - INFO - Loading 100000 records into fact_applications\n",
      "2025-08-16 22:04:16,434 - INFO - Successfully loaded applications\n",
      "2025-08-16 22:04:21,313 - INFO - Loading 50000 records into fact_predictions\n",
      "2025-08-16 22:11:04,384 - INFO - Successfully loaded predictions\n",
      "2025-08-16 22:11:04,386 - INFO - Loading 100000 records into fact_transactions\n",
      "2025-08-16 22:16:54,344 - INFO - Successfully loaded transactions\n",
      "2025-08-16 22:16:54,851 - INFO - dim_customer_profiles: 25000 records loaded\n",
      "2025-08-16 22:16:55,077 - INFO - dim_bureau: 25000 records loaded\n",
      "2025-08-16 22:16:55,390 - INFO - fact_applications: 100000 records loaded\n",
      "2025-08-16 22:16:55,655 - INFO - fact_predictions: 50000 records loaded\n",
      "2025-08-16 22:16:55,995 - INFO - fact_transactions: 100000 records loaded\n",
      "2025-08-16 22:16:56,218 - INFO - Data pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = DataPipeline()\n",
    "    # Set force_recreate=True to drop and recreate tables with new constraints\n",
    "    pipeline.run_pipeline(force_recreate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
